{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7c490c0",
   "metadata": {},
   "source": [
    "# An√°lise de Readmiss√£o Hospitalar de Pacientes Diab√©ticos\n",
    "\n",
    "## Projeto de Predi√ß√£o de Readmiss√£o Hospitalar em Menos de 30 Dias\n",
    "\n",
    "Este notebook implementa um pipeline completo de an√°lise e pr√©-processamento de dados para predi√ß√£o de readmiss√£o hospitalar de pacientes diab√©ticos em menos de 30 dias.\n",
    "\n",
    "### Objetivos:\n",
    "1. **An√°lise Explorat√≥ria**: Compreender a estrutura e qualidade dos dados\n",
    "2. **Limpeza de Dados**: Remover registros inv√°lidos e tratar dados faltantes\n",
    "3. **Engenharia de Features**: Preparar dados para algoritmos de Machine Learning\n",
    "4. **Prepara√ß√£o para Modelagem**: Dividir dados em conjuntos de treino e teste\n",
    "\n",
    "### Dataset:\n",
    "- **Fonte**: UCI Machine Learning Repository - Diabetes 130-US hospitals for years 1999-2008 Data Set\n",
    "- **Registros**: 101,766 encontros hospitalares\n",
    "- **Features**: 50 vari√°veis (demogr√°ficas, cl√≠nicas, medicamentos)\n",
    "- **Target**: Readmiss√£o em <30 dias, >30 dias, ou n√£o readmitido\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3fae0c",
   "metadata": {},
   "source": [
    "## 1. Importar Bibliotecas\n",
    "\n",
    "Importando todas as bibliotecas necess√°rias para an√°lise de dados, visualiza√ß√£o e machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fbb004e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bibliotecas para manipula√ß√£o de dados\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Bibliotecas para visualiza√ß√£o\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Bibliotecas para machine learning\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Configura√ß√µes para visualiza√ß√£o\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "# Configura√ß√µes do pandas\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', 50)\n",
    "\n",
    "# Importar m√≥dulos customizados do projeto\n",
    "sys.path.append('../src')\n",
    "try:\n",
    "    from config import *\n",
    "    print(\"‚úÖ M√≥dulos do projeto importados com sucesso\")\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è M√≥dulos do projeto n√£o encontrados, usando caminhos relativos\")\n",
    "    \n",
    "print(\"üìö Todas as bibliotecas importadas com sucesso!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf8dd39",
   "metadata": {},
   "source": [
    "## 2. An√°lise Explorat√≥ria dos Dados Processados\n",
    "\n",
    "Carregando o dataset original e realizando explora√ß√£o inicial para compreender a estrutura dos dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb80012",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregando o dataset original\n",
    "data_path = '../data/diabetic_data.csv'\n",
    "df_raw = pd.read_csv(data_path)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"INFORMA√á√ïES B√ÅSICAS DO DATASET\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nüìä Tamanho do dataset: {df_raw.shape}\")\n",
    "print(f\"üìã N√∫mero de colunas: {df_raw.shape[1]}\")\n",
    "print(f\"üìà N√∫mero de registros: {df_raw.shape[0]}\")\n",
    "print(f\"üë• N√∫mero de pacientes √∫nicos: {df_raw['patient_nbr'].nunique()}\")\n",
    "print(f\"üè• N√∫mero de encontros hospitalares: {len(df_raw)}\")\n",
    "\n",
    "print(f\"\\nüìã Tipos de dados por coluna:\")\n",
    "print(df_raw.dtypes.value_counts())\n",
    "\n",
    "print(f\"\\nüîç Primeiras 5 linhas do dataset:\")\n",
    "display(df_raw.head())\n",
    "\n",
    "print(f\"\\nüìà Estat√≠sticas descritivas das colunas num√©ricas:\")\n",
    "display(df_raw.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693a8f9f",
   "metadata": {},
   "source": [
    "## 3. Avalia√ß√£o da Qualidade dos Dados\n",
    "\n",
    "Analisando a qualidade dos dados, identificando valores faltantes, dados inconsistentes e problemas que precisam ser tratados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b52e7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"AN√ÅLISE DE QUALIDADE DOS DADOS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 1. Verificar valores nulos tradicionais\n",
    "print(\"\\nüîç An√°lise de valores nulos (NaN):\")\n",
    "null_counts = df_raw.isnull().sum()\n",
    "if null_counts.sum() > 0:\n",
    "    null_summary = pd.DataFrame({\n",
    "        'Coluna': null_counts.index,\n",
    "        'Valores_Nulos': null_counts.values,\n",
    "        'Percentual': (null_counts.values / len(df_raw)) * 100\n",
    "    })\n",
    "    null_summary = null_summary[null_summary['Valores_Nulos'] > 0].sort_values('Valores_Nulos', ascending=False)\n",
    "    display(null_summary)\n",
    "else:\n",
    "    print(\"‚úÖ Nenhum valor nulo (NaN) encontrado\")\n",
    "\n",
    "# 2. Verificar valores '?' (representam dados faltantes neste dataset)\n",
    "print(\"\\nüîç An√°lise de valores faltantes ('?'):\")\n",
    "missing_summary = []\n",
    "for col in df_raw.columns:\n",
    "    if df_raw[col].dtype == 'object':\n",
    "        missing_count = (df_raw[col] == '?').sum()\n",
    "        if missing_count > 0:\n",
    "            missing_pct = (missing_count / len(df_raw)) * 100\n",
    "            missing_summary.append({\n",
    "                'Coluna': col,\n",
    "                'Valores_Faltantes': missing_count,\n",
    "                'Percentual': f\"{missing_pct:.1f}%\"\n",
    "            })\n",
    "\n",
    "if missing_summary:\n",
    "    missing_df = pd.DataFrame(missing_summary)\n",
    "    missing_df = missing_df.sort_values('Valores_Faltantes', ascending=False)\n",
    "    display(missing_df)\n",
    "    \n",
    "    # Visualizar colunas com mais dados faltantes\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    top_missing = missing_df.head(10)\n",
    "    plt.barh(top_missing['Coluna'], top_missing['Valores_Faltantes'])\n",
    "    plt.title('Top 10 Colunas com Mais Dados Faltantes')\n",
    "    plt.xlabel('N√∫mero de Valores Faltantes')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"‚úÖ Nenhum valor '?' encontrado\")\n",
    "\n",
    "# 3. Verificar distribui√ß√£o das colunas categ√≥ricas principais\n",
    "print(\"\\nüîç Distribui√ß√£o das principais colunas categ√≥ricas:\")\n",
    "categorical_cols = ['race', 'gender', 'age', 'medical_specialty', 'readmitted']\n",
    "\n",
    "for col in categorical_cols:\n",
    "    if col in df_raw.columns:\n",
    "        print(f\"\\nüìã {col}:\")\n",
    "        value_counts = df_raw[col].value_counts()\n",
    "        print(value_counts.head(10))\n",
    "        if '?' in value_counts.index:\n",
    "            missing_pct = (value_counts['?'] / len(df_raw)) * 100\n",
    "            print(f\"   ‚ùó Dados faltantes ('?'): {value_counts['?']} ({missing_pct:.1f}%)\")\n",
    "\n",
    "# 4. Verificar duplicatas\n",
    "print(f\"\\nüîç An√°lise de duplicatas:\")\n",
    "print(f\"Registros totais: {len(df_raw)}\")\n",
    "print(f\"Registros √∫nicos por encounter_id: {df_raw['encounter_id'].nunique()}\")\n",
    "print(f\"Pacientes √∫nicos: {df_raw['patient_nbr'].nunique()}\")\n",
    "print(f\"Pacientes com m√∫ltiplas interna√ß√µes: {len(df_raw) - df_raw['patient_nbr'].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0fb70e",
   "metadata": {},
   "source": [
    "## 4. An√°lise e Engenharia da Vari√°vel Alvo\n",
    "\n",
    "Analisando a vari√°vel alvo 'readmitted' e criando uma vari√°vel bin√°ria para predi√ß√£o de readmiss√£o em menos de 30 dias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c944725f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"AN√ÅLISE DA VARI√ÅVEL ALVO\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Analisar distribui√ß√£o original da vari√°vel readmitted\n",
    "print(\"\\nüéØ Distribui√ß√£o da vari√°vel 'readmitted':\")\n",
    "readmitted_counts = df_raw['readmitted'].value_counts()\n",
    "readmitted_pct = df_raw['readmitted'].value_counts(normalize=True) * 100\n",
    "\n",
    "readmitted_summary = pd.DataFrame({\n",
    "    'Categoria': readmitted_counts.index,\n",
    "    'Contagem': readmitted_counts.values,\n",
    "    'Percentual': readmitted_pct.values.round(2)\n",
    "})\n",
    "display(readmitted_summary)\n",
    "\n",
    "# Visualizar distribui√ß√£o\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Gr√°fico de barras\n",
    "readmitted_counts.plot(kind='bar', ax=ax1, color=['skyblue', 'lightcoral', 'lightgreen'])\n",
    "ax1.set_title('Distribui√ß√£o da Vari√°vel Readmitted')\n",
    "ax1.set_xlabel('Categoria de Readmiss√£o')\n",
    "ax1.set_ylabel('N√∫mero de Pacientes')\n",
    "ax1.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Gr√°fico de pizza\n",
    "ax2.pie(readmitted_counts.values, labels=readmitted_counts.index, autopct='%1.1f%%', \n",
    "        colors=['skyblue', 'lightcoral', 'lightgreen'])\n",
    "ax2.set_title('Distribui√ß√£o Percentual da Readmiss√£o')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Criar vari√°vel alvo bin√°ria\n",
    "print(\"\\nüîß Criando vari√°vel alvo bin√°ria:\")\n",
    "print(\"Defini√ß√£o: readmitted == '<30' ‚Üí 1 (readmitido em <30 dias)\")\n",
    "print(\"           readmitted == '>30' ou 'NO' ‚Üí 0 (n√£o readmitido em <30 dias)\")\n",
    "\n",
    "df_raw['target'] = df_raw['readmitted'].apply(lambda x: 1 if x == '<30' else 0)\n",
    "\n",
    "print(f\"\\nüìä Distribui√ß√£o da vari√°vel alvo bin√°ria:\")\n",
    "target_counts = df_raw['target'].value_counts()\n",
    "target_pct = df_raw['target'].value_counts(normalize=True) * 100\n",
    "\n",
    "target_summary = pd.DataFrame({\n",
    "    'Classe': ['N√£o Readmitido <30 dias', 'Readmitido <30 dias'],\n",
    "    'Contagem': [target_counts[0], target_counts[1]],\n",
    "    'Percentual': [target_pct[0].round(2), target_pct[1].round(2)]\n",
    "})\n",
    "display(target_summary)\n",
    "\n",
    "print(f\"\\nüìà M√©tricas da vari√°vel alvo:\")\n",
    "print(f\"Propor√ß√£o de readmiss√µes em <30 dias: {df_raw['target'].mean():.3f}\")\n",
    "print(f\"Raz√£o de desbalanceamento: {target_counts[0] / target_counts[1]:.1f}:1\")\n",
    "\n",
    "# Verificar se h√° valores nulos na vari√°vel alvo\n",
    "null_readmitted = df_raw['readmitted'].isnull().sum()\n",
    "null_target = df_raw['target'].isnull().sum()\n",
    "print(f\"\\nüîç Valores nulos:\")\n",
    "print(f\"Valores nulos em 'readmitted': {null_readmitted}\")\n",
    "print(f\"Valores nulos em 'target': {null_target}\")\n",
    "\n",
    "if null_readmitted == 0 and null_target == 0:\n",
    "    print(\"‚úÖ Nenhum valor nulo encontrado nas vari√°veis alvo\")\n",
    "else:\n",
    "    print(\"‚ùó Aten√ß√£o: valores nulos encontrados nas vari√°veis alvo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b50583",
   "metadata": {},
   "source": [
    "## 5. Pipeline de Limpeza de Dados\n",
    "\n",
    "Implementando pipeline sistem√°tico de limpeza de dados seguindo as melhores pr√°ticas para an√°lise de readmiss√£o hospitalar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817907cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"PIPELINE DE LIMPEZA DOS DADOS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Fazer uma c√≥pia dos dados para limpeza\n",
    "df_clean = df_raw.copy()\n",
    "print(f\"\\nüìä Dados iniciais: {df_clean.shape}\")\n",
    "\n",
    "# ETAPA 1: Remover pacientes expirados (falecidos)\n",
    "print(\"\\nüîß ETAPA 1: Removendo pacientes expirados\")\n",
    "print(\"C√≥digos de discharge_disposition_id para 'Expired': 11, 19, 20, 21\")\n",
    "\n",
    "expired_codes = [11, 19, 20, 21]\n",
    "expired_count = df_clean['discharge_disposition_id'].isin(expired_codes).sum()\n",
    "print(f\"Registros com pacientes expirados: {expired_count}\")\n",
    "\n",
    "df_clean = df_clean[~df_clean['discharge_disposition_id'].isin(expired_codes)].copy()\n",
    "print(f\"Registros ap√≥s remo√ß√£o: {df_clean.shape[0]}\")\n",
    "print(f\"Registros removidos: {len(df_raw) - len(df_clean)}\")\n",
    "\n",
    "# ETAPA 2: Tratar dados faltantes\n",
    "print(\"\\nüîß ETAPA 2: Tratando dados faltantes\")\n",
    "\n",
    "# Verificar colunas com alto percentual de dados faltantes\n",
    "columns_to_analyze = ['weight', 'payer_code', 'medical_specialty']\n",
    "missing_analysis = {}\n",
    "\n",
    "for col in columns_to_analyze:\n",
    "    if col in df_clean.columns:\n",
    "        missing_count = (df_clean[col] == '?').sum()\n",
    "        missing_pct = (missing_count / len(df_clean)) * 100\n",
    "        missing_analysis[col] = {'count': missing_count, 'percentage': missing_pct}\n",
    "        print(f\"Dados faltantes em '{col}': {missing_count} ({missing_pct:.1f}%)\")\n",
    "\n",
    "# Remover colunas com muitos dados faltantes (weight e payer_code)\n",
    "columns_to_drop = ['weight', 'payer_code']\n",
    "print(f\"\\nRemo√ß√£o de colunas com muitos dados faltantes: {columns_to_drop}\")\n",
    "df_clean = df_clean.drop(columns_to_drop, axis=1)\n",
    "\n",
    "# Tratar medical_specialty: substituir '?' por 'missing'\n",
    "if 'medical_specialty' in df_clean.columns:\n",
    "    missing_specialty_before = (df_clean['medical_specialty'] == '?').sum()\n",
    "    df_clean['medical_specialty'] = df_clean['medical_specialty'].replace('?', 'missing')\n",
    "    missing_specialty_after = (df_clean['medical_specialty'] == 'missing').sum()\n",
    "    print(f\"Valores '?' em medical_specialty substitu√≠dos por 'missing': {missing_specialty_after}\")\n",
    "\n",
    "# ETAPA 3: Remover duplicatas de pacientes (manter apenas primeira interna√ß√£o)\n",
    "print(\"\\nüîß ETAPA 3: Removendo dados duplicados de pacientes\")\n",
    "print(f\"Registros antes da remo√ß√£o de duplicatas: {len(df_clean)}\")\n",
    "print(f\"Pacientes √∫nicos: {df_clean['patient_nbr'].nunique()}\")\n",
    "\n",
    "# Manter apenas a primeira interna√ß√£o de cada paciente\n",
    "df_clean = df_clean.drop_duplicates(subset=['patient_nbr'], keep='first')\n",
    "\n",
    "print(f\"Registros ap√≥s remo√ß√£o de duplicatas: {len(df_clean)}\")\n",
    "print(f\"Pacientes √∫nicos ap√≥s limpeza: {df_clean['patient_nbr'].nunique()}\")\n",
    "print(f\"Registros de duplicatas removidos: {len(df_raw) - len(df_clean)}\")\n",
    "\n",
    "# ETAPA 4: Verificar distribui√ß√£o da vari√°vel alvo ap√≥s limpeza\n",
    "print(\"\\nüîß ETAPA 4: Verificando distribui√ß√£o da vari√°vel alvo ap√≥s limpeza\")\n",
    "print(\"Distribui√ß√£o da vari√°vel 'readmitted' ap√≥s limpeza:\")\n",
    "print(df_clean['readmitted'].value_counts())\n",
    "\n",
    "print(f\"\\nDistribui√ß√£o da vari√°vel 'target' ap√≥s limpeza:\")\n",
    "print(df_clean['target'].value_counts())\n",
    "print(f\"Propor√ß√£o de readmiss√µes em <30 dias ap√≥s limpeza: {df_clean['target'].mean():.3f}\")\n",
    "\n",
    "# Resumo final da limpeza\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RESUMO DA LIMPEZA DOS DADOS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"üìä Registros originais: {len(df_raw):,}\")\n",
    "print(f\"üìä Registros ap√≥s limpeza: {len(df_clean):,}\")\n",
    "print(f\"üìä Registros removidos: {len(df_raw) - len(df_clean):,}\")\n",
    "print(f\"üìä Percentual mantido: {(len(df_clean) / len(df_raw)) * 100:.1f}%\")\n",
    "print(f\"üìä Colunas originais: {df_raw.shape[1]}\")\n",
    "print(f\"üìä Colunas ap√≥s limpeza: {df_clean.shape[1]}\")\n",
    "print(f\"üìä Dimens√µes finais: {df_clean.shape}\")\n",
    "\n",
    "# Visualizar impacto da limpeza na vari√°vel alvo\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Antes da limpeza\n",
    "target_before = df_raw['target'].value_counts()\n",
    "ax1.pie(target_before.values, labels=['N√£o Readmitido <30d', 'Readmitido <30d'], \n",
    "        autopct='%1.1f%%', colors=['skyblue', 'lightcoral'])\n",
    "ax1.set_title(f'Distribui√ß√£o da Vari√°vel Alvo\\nAntes da Limpeza (n={len(df_raw):,})')\n",
    "\n",
    "# Ap√≥s a limpeza\n",
    "target_after = df_clean['target'].value_counts()\n",
    "ax2.pie(target_after.values, labels=['N√£o Readmitido <30d', 'Readmitido <30d'], \n",
    "        autopct='%1.1f%%', colors=['skyblue', 'lightcoral'])\n",
    "ax2.set_title(f'Distribui√ß√£o da Vari√°vel Alvo\\nAp√≥s a Limpeza (n={len(df_clean):,})')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Limpeza dos dados conclu√≠da com sucesso!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbaaadb0",
   "metadata": {},
   "source": [
    "## 6. Engenharia de Atributos e Codifica√ß√£o\n",
    "\n",
    "Preparando as features para algoritmos de Machine Learning atrav√©s de encoding de vari√°veis categ√≥ricas e engenharia de features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f17d142",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"ENGENHARIA DE FEATURES E ENCODING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Preparar dados para modelagem\n",
    "df_processed = df_clean.copy()\n",
    "\n",
    "# ETAPA 1: Remover colunas desnecess√°rias para modelagem\n",
    "print(\"\\nüîß ETAPA 1: Removendo colunas desnecess√°rias para modelagem\")\n",
    "cols_to_drop = ['encounter_id', 'patient_nbr', 'readmitted', 'diag_1', 'diag_2', 'diag_3']\n",
    "print(f\"Colunas a serem removidas: {cols_to_drop}\")\n",
    "\n",
    "# Verificar quais colunas existem antes de remover\n",
    "existing_cols_to_drop = [col for col in cols_to_drop if col in df_processed.columns]\n",
    "print(f\"Colunas existentes que ser√£o removidas: {existing_cols_to_drop}\")\n",
    "\n",
    "df_processed = df_processed.drop(existing_cols_to_drop, axis=1)\n",
    "print(f\"Dimens√µes ap√≥s remo√ß√£o: {df_processed.shape}\")\n",
    "\n",
    "# ETAPA 2: Identificar tipos de colunas\n",
    "print(\"\\nüîß ETAPA 2: Identificando tipos de colunas\")\n",
    "numeric_cols = df_processed.select_dtypes(include=np.number).columns.tolist()\n",
    "categorical_cols = df_processed.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "# Remover target da lista de features num√©ricas\n",
    "if 'target' in numeric_cols:\n",
    "    numeric_cols.remove('target')\n",
    "\n",
    "print(f\"üìä Colunas num√©ricas ({len(numeric_cols)}): {numeric_cols[:10]}{'...' if len(numeric_cols) > 10 else ''}\")\n",
    "print(f\"üìä Colunas categ√≥ricas ({len(categorical_cols)}): {categorical_cols}\")\n",
    "\n",
    "# Analisar colunas categ√≥ricas em detalhes\n",
    "print(f\"\\nüìã An√°lise detalhada das colunas categ√≥ricas:\")\n",
    "for col in categorical_cols:\n",
    "    unique_values = df_processed[col].nunique()\n",
    "    sample_values = df_processed[col].unique()[:5]\n",
    "    print(f\"  {col}: {unique_values} valores √∫nicos, exemplos: {sample_values}\")\n",
    "\n",
    "# ETAPA 3: Aplicar One-Hot Encoding nas colunas categ√≥ricas\n",
    "print(f\"\\nüîß ETAPA 3: Aplicando One-Hot Encoding\")\n",
    "print(f\"Aplicando encoding em {len(categorical_cols)} colunas categ√≥ricas\")\n",
    "print(f\"Dimens√µes antes do encoding: {df_processed.shape}\")\n",
    "\n",
    "df_encoded = pd.get_dummies(df_processed, columns=categorical_cols, drop_first=True)\n",
    "\n",
    "print(f\"Dimens√µes ap√≥s encoding: {df_encoded.shape}\")\n",
    "print(f\"Novas features criadas: {df_encoded.shape[1] - df_processed.shape[1]}\")\n",
    "\n",
    "# ETAPA 4: Separar features (X) e vari√°vel alvo (y)\n",
    "print(f\"\\nüîß ETAPA 4: Separando features (X) e vari√°vel alvo (y)\")\n",
    "\n",
    "X = df_encoded.drop('target', axis=1)\n",
    "y = df_encoded['target']\n",
    "\n",
    "print(f\"üìä Formato de X (features): {X.shape}\")\n",
    "print(f\"üìä Formato de y (target): {y.shape}\")\n",
    "print(f\"üìä N√∫mero total de features: {X.shape[1]}\")\n",
    "\n",
    "# ETAPA 5: An√°lise das features criadas\n",
    "print(f\"\\nüîß ETAPA 5: An√°lise das features criadas\")\n",
    "\n",
    "# Categorizar features por tipo\n",
    "medication_cols = [col for col in X.columns if any(med in col.lower() for med in \n",
    "                  ['metformin', 'insulin', 'glyburide', 'glipizide', 'glimepiride', 'pioglitazone', \n",
    "                   'rosiglitazone', 'acarbose', 'miglitol', 'troglitazone', 'tolazamide', \n",
    "                   'examide', 'citoglipton', 'tolbutamide', 'repaglinide', 'nateglinide', \n",
    "                   'chlorpropamide', 'acetohexamide'])]\n",
    "\n",
    "demographic_cols = [col for col in X.columns if any(demo in col.lower() for demo in \n",
    "                   ['race', 'gender', 'age'])]\n",
    "\n",
    "admission_cols = [col for col in X.columns if any(adm in col.lower() for adm in \n",
    "                 ['admission_type', 'discharge_disposition', 'admission_source'])]\n",
    "\n",
    "lab_cols = [col for col in X.columns if any(lab in col.lower() for lab in \n",
    "           ['num_lab_procedures', 'num_procedures', 'num_medications', 'time_in_hospital'])]\n",
    "\n",
    "print(f\"üìä Features de medicamentos: {len(medication_cols)}\")\n",
    "print(f\"üìä Features demogr√°ficas: {len(demographic_cols)}\")\n",
    "print(f\"üìä Features de admiss√£o: {len(admission_cols)}\")\n",
    "print(f\"üìä Features laboratoriais/cl√≠nicas: {len(lab_cols)}\")\n",
    "print(f\"üìä Outras features: {X.shape[1] - len(medication_cols) - len(demographic_cols) - len(admission_cols) - len(lab_cols)}\")\n",
    "\n",
    "# Verificar estat√≠sticas b√°sicas das features num√©ricas originais\n",
    "print(f\"\\nüìà Estat√≠sticas das features num√©ricas principais:\")\n",
    "numeric_features_summary = X[numeric_cols].describe()\n",
    "display(numeric_features_summary)\n",
    "\n",
    "# Visualizar distribui√ß√£o de algumas features importantes\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Time in hospital\n",
    "axes[0,0].hist(X['time_in_hospital'], bins=20, alpha=0.7, color='skyblue')\n",
    "axes[0,0].set_title('Distribui√ß√£o: Tempo de Interna√ß√£o')\n",
    "axes[0,0].set_xlabel('Dias')\n",
    "\n",
    "# Number of lab procedures\n",
    "axes[0,1].hist(X['num_lab_procedures'], bins=20, alpha=0.7, color='lightgreen')\n",
    "axes[0,1].set_title('Distribui√ß√£o: N√∫mero de Procedimentos Laboratoriais')\n",
    "axes[0,1].set_xlabel('N√∫mero de Procedimentos')\n",
    "\n",
    "# Number of medications\n",
    "axes[1,0].hist(X['num_medications'], bins=20, alpha=0.7, color='lightcoral')\n",
    "axes[1,0].set_title('Distribui√ß√£o: N√∫mero de Medicamentos')\n",
    "axes[1,0].set_xlabel('N√∫mero de Medicamentos')\n",
    "\n",
    "# Number of diagnoses\n",
    "axes[1,1].hist(X['number_diagnoses'], bins=20, alpha=0.7, color='lightyellow')\n",
    "axes[1,1].set_title('Distribui√ß√£o: N√∫mero de Diagn√≥sticos')\n",
    "axes[1,1].set_xlabel('N√∫mero de Diagn√≥sticos')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Engenharia de features conclu√≠da com sucesso!\")\n",
    "print(f\"‚úÖ Dataset pronto para Machine Learning: {X.shape[0]} amostras, {X.shape[1]} features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc0d839",
   "metadata": {},
   "source": [
    "## 7. Divis√£o dos Dados para Treinamento do Modelo\n",
    "\n",
    "Dividindo os dados em conjuntos de treino e teste usando amostragem estratificada para manter o equil√≠brio das classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d19212",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"DIVIS√ÉO DOS DADOS PARA TREINAMENTO\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Configura√ß√µes da divis√£o\n",
    "test_size = 0.2\n",
    "random_state = 42\n",
    "\n",
    "print(f\"\\n‚öôÔ∏è Configura√ß√µes da divis√£o:\")\n",
    "print(f\"Propor√ß√£o de teste: {test_size} ({test_size*100}%)\")\n",
    "print(f\"Propor√ß√£o de treino: {1-test_size} ({(1-test_size)*100}%)\")\n",
    "print(f\"Random state: {random_state} (para reprodutibilidade)\")\n",
    "print(f\"Estratifica√ß√£o: Sim (mant√©m propor√ß√£o de classes)\")\n",
    "\n",
    "# Verificar distribui√ß√£o antes da divis√£o\n",
    "print(f\"\\nüìä Distribui√ß√£o da vari√°vel alvo antes da divis√£o:\")\n",
    "target_distribution = y.value_counts()\n",
    "target_distribution_pct = y.value_counts(normalize=True) * 100\n",
    "print(f\"Classe 0 (N√£o readmitido <30d): {target_distribution[0]} ({target_distribution_pct[0]:.1f}%)\")\n",
    "print(f\"Classe 1 (Readmitido <30d): {target_distribution[1]} ({target_distribution_pct[1]:.1f}%)\")\n",
    "\n",
    "# Dividir os dados\n",
    "print(f\"\\nüîß Executando divis√£o dos dados...\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=test_size,\n",
    "    random_state=random_state,\n",
    "    stratify=y  # MUITO importante para datasets desbalanceados\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Divis√£o conclu√≠da com sucesso!\")\n",
    "\n",
    "# Verificar tamanhos dos conjuntos\n",
    "print(f\"\\nüìä Tamanhos dos conjuntos criados:\")\n",
    "print(f\"X_train: {X_train.shape} (amostras: {X_train.shape[0]}, features: {X_train.shape[1]})\")\n",
    "print(f\"X_test:  {X_test.shape} (amostras: {X_test.shape[0]}, features: {X_test.shape[1]})\")\n",
    "print(f\"y_train: {y_train.shape} (amostras: {y_train.shape[0]})\")\n",
    "print(f\"y_test:  {y_test.shape} (amostras: {y_test.shape[0]})\")\n",
    "\n",
    "# Verificar distribui√ß√£o das classes nos conjuntos\n",
    "print(f\"\\nüìä Distribui√ß√£o das classes nos conjuntos:\")\n",
    "\n",
    "# Conjunto de treino\n",
    "train_distribution = y_train.value_counts()\n",
    "train_distribution_pct = y_train.value_counts(normalize=True) * 100\n",
    "print(f\"\\nConjunto de TREINO:\")\n",
    "print(f\"  Classe 0: {train_distribution[0]} ({train_distribution_pct[0]:.1f}%)\")\n",
    "print(f\"  Classe 1: {train_distribution[1]} ({train_distribution_pct[1]:.1f}%)\")\n",
    "\n",
    "# Conjunto de teste\n",
    "test_distribution = y_test.value_counts()\n",
    "test_distribution_pct = y_test.value_counts(normalize=True) * 100\n",
    "print(f\"\\nConjunto de TESTE:\")\n",
    "print(f\"  Classe 0: {test_distribution[0]} ({test_distribution_pct[0]:.1f}%)\")\n",
    "print(f\"  Classe 1: {test_distribution[1]} ({test_distribution_pct[1]:.1f}%)\")\n",
    "\n",
    "# Visualizar distribui√ß√£o dos conjuntos\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# Dataset completo\n",
    "y.value_counts().plot(kind='bar', ax=ax1, color=['skyblue', 'lightcoral'])\n",
    "ax1.set_title(f'Dataset Completo\\n(n={len(y):,})')\n",
    "ax1.set_xlabel('Classe')\n",
    "ax1.set_ylabel('N√∫mero de Amostras')\n",
    "ax1.set_xticklabels(['N√£o Readmitido <30d', 'Readmitido <30d'], rotation=45)\n",
    "\n",
    "# Conjunto de treino\n",
    "y_train.value_counts().plot(kind='bar', ax=ax2, color=['skyblue', 'lightcoral'])\n",
    "ax2.set_title(f'Conjunto de Treino\\n(n={len(y_train):,})')\n",
    "ax2.set_xlabel('Classe')\n",
    "ax2.set_ylabel('N√∫mero de Amostras')\n",
    "ax2.set_xticklabels(['N√£o Readmitido <30d', 'Readmitido <30d'], rotation=45)\n",
    "\n",
    "# Conjunto de teste\n",
    "y_test.value_counts().plot(kind='bar', ax=ax3, color=['skyblue', 'lightcoral'])\n",
    "ax3.set_title(f'Conjunto de Teste\\n(n={len(y_test):,})')\n",
    "ax3.set_xlabel('Classe')\n",
    "ax3.set_ylabel('N√∫mero de Amostras')\n",
    "ax3.set_xticklabels(['N√£o Readmitido <30d', 'Readmitido <30d'], rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Verificar se a estratifica√ß√£o funcionou\n",
    "print(f\"\\n‚úÖ Verifica√ß√£o da estratifica√ß√£o:\")\n",
    "original_ratio = target_distribution_pct[1]\n",
    "train_ratio = train_distribution_pct[1]\n",
    "test_ratio = test_distribution_pct[1]\n",
    "\n",
    "print(f\"Taxa de readmiss√£o original: {original_ratio:.1f}%\")\n",
    "print(f\"Taxa de readmiss√£o no treino: {train_ratio:.1f}%\")\n",
    "print(f\"Taxa de readmiss√£o no teste: {test_ratio:.1f}%\")\n",
    "\n",
    "# Verificar se as diferen√ßas s√£o pequenas (boa estratifica√ß√£o)\n",
    "diff_train = abs(original_ratio - train_ratio)\n",
    "diff_test = abs(original_ratio - test_ratio)\n",
    "\n",
    "if diff_train < 1.0 and diff_test < 1.0:\n",
    "    print(\"‚úÖ Estratifica√ß√£o bem-sucedida: distribui√ß√µes similares entre conjuntos\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Aten√ß√£o: diferen√ßas na distribui√ß√£o podem afetar a avalia√ß√£o\")\n",
    "\n",
    "print(f\"\\nüéØ DADOS PRONTOS PARA MACHINE LEARNING:\")\n",
    "print(f\"üìä Conjunto de treino: {X_train.shape[0]:,} amostras, {X_train.shape[1]} features\")\n",
    "print(f\"üìä Conjunto de teste: {X_test.shape[0]:,} amostras, {X_test.shape[1]} features\")\n",
    "print(f\"üìä Taxa de readmiss√£o balanceada entre conjuntos\")\n",
    "print(f\"üìä Dados prontos para treinamento de modelos!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d725509",
   "metadata": {},
   "source": [
    "## 8. Exporta√ß√£o dos Dados Processados\n",
    "\n",
    "Salvando os datasets limpos e processados para uso posterior em modelagem e an√°lise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f7099f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"EXPORTANDO DADOS PROCESSADOS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Definir caminhos de sa√≠da\n",
    "output_dir = '../data/'\n",
    "clean_data_path = os.path.join(output_dir, 'diabetic_data_clean.csv')\n",
    "processed_data_path = os.path.join(output_dir, 'diabetic_data_processed.csv')\n",
    "\n",
    "# Caminhos para conjuntos de treino e teste\n",
    "train_features_path = os.path.join(output_dir, 'X_train.csv')\n",
    "test_features_path = os.path.join(output_dir, 'X_test.csv')\n",
    "train_target_path = os.path.join(output_dir, 'y_train.csv')\n",
    "test_target_path = os.path.join(output_dir, 'y_test.csv')\n",
    "\n",
    "print(f\"\\nüíæ Salvando datasets...\")\n",
    "\n",
    "# 1. Salvar dados limpos (intermedi√°rio)\n",
    "df_clean.to_csv(clean_data_path, index=False)\n",
    "print(f\"‚úÖ Dados limpos salvos: {clean_data_path}\")\n",
    "print(f\"   Dimens√µes: {df_clean.shape}\")\n",
    "\n",
    "# 2. Salvar dados processados completos (com encoding)\n",
    "df_encoded.to_csv(processed_data_path, index=False)\n",
    "print(f\"‚úÖ Dados processados salvos: {processed_data_path}\")\n",
    "print(f\"   Dimens√µes: {df_encoded.shape}\")\n",
    "\n",
    "# 3. Salvar conjuntos de treino e teste separadamente\n",
    "X_train.to_csv(train_features_path, index=False)\n",
    "X_test.to_csv(test_features_path, index=False)\n",
    "y_train.to_csv(train_target_path, index=False, header=['target'])\n",
    "y_test.to_csv(test_target_path, index=False, header=['target'])\n",
    "\n",
    "print(f\"‚úÖ Conjuntos de Machine Learning salvos:\")\n",
    "print(f\"   Features de treino: {train_features_path} {X_train.shape}\")\n",
    "print(f\"   Features de teste: {test_features_path} {X_test.shape}\")\n",
    "print(f\"   Target de treino: {train_target_path} {y_train.shape}\")\n",
    "print(f\"   Target de teste: {test_target_path} {y_test.shape}\")\n",
    "\n",
    "# 4. Criar arquivo de metadados com informa√ß√µes do processamento\n",
    "metadata = {\n",
    "    'pipeline_info': {\n",
    "        'data_original_shape': df_raw.shape,\n",
    "        'data_clean_shape': df_clean.shape,\n",
    "        'data_processed_shape': df_encoded.shape,\n",
    "        'train_shape': X_train.shape,\n",
    "        'test_shape': X_test.shape,\n",
    "        'target_distribution': {\n",
    "            'total_readmitted_30d': int(y.sum()),\n",
    "            'total_not_readmitted_30d': int(len(y) - y.sum()),\n",
    "            'train_readmitted_30d': int(y_train.sum()),\n",
    "            'test_readmitted_30d': int(y_test.sum()),\n",
    "            'readmission_rate': float(y.mean())\n",
    "        }\n",
    "    },\n",
    "    'processing_steps': {\n",
    "        'expired_patients_removed': int(len(df_raw) - len(df_clean)),\n",
    "        'columns_dropped': existing_cols_to_drop,\n",
    "        'missing_data_handled': ['weight', 'payer_code', 'medical_specialty'],\n",
    "        'duplicates_removed': 'kept_first_encounter_per_patient',\n",
    "        'encoding_applied': 'one_hot_encoding_categorical_vars',\n",
    "        'total_features_created': int(X.shape[1])\n",
    "    },\n",
    "    'data_quality': {\n",
    "        'missing_values_remaining': int(df_encoded.isnull().sum().sum()),\n",
    "        'categorical_encoded': len(categorical_cols),\n",
    "        'numeric_features': len(numeric_cols),\n",
    "        'total_features': int(X.shape[1])\n",
    "    }\n",
    "}\n",
    "\n",
    "# Salvar metadados\n",
    "import json\n",
    "metadata_path = os.path.join(output_dir, 'processing_metadata.json')\n",
    "with open(metadata_path, 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "print(f\"‚úÖ Metadados salvos: {metadata_path}\")\n",
    "\n",
    "# 5. Criar resumo executivo do processamento\n",
    "summary_path = os.path.join(output_dir, 'data_processing_summary.txt')\n",
    "with open(summary_path, 'w', encoding='utf-8') as f:\n",
    "    f.write(\"RESUMO DO PROCESSAMENTO DE DADOS - READMISS√ÉO HOSPITALAR DIAB√âTICA\\\\n\")\n",
    "    f.write(\"=\"*70 + \"\\\\n\\\\n\")\n",
    "    \n",
    "    f.write(f\"Data de processamento: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\\\\n\\\\n\")\n",
    "    \n",
    "    f.write(\"TRANSFORMA√á√ïES APLICADAS:\\\\n\")\n",
    "    f.write(f\"1. Dados originais: {df_raw.shape[0]:,} registros, {df_raw.shape[1]} colunas\\\\n\")\n",
    "    f.write(f\"2. Remo√ß√£o de pacientes expirados: -{len(df_raw) - len(df_clean):,} registros\\\\n\")\n",
    "    f.write(f\"3. Tratamento de dados faltantes: removidas colunas weight, payer_code\\\\n\")\n",
    "    f.write(f\"4. Remo√ß√£o de duplicatas por paciente: mantida primeira interna√ß√£o\\\\n\")\n",
    "    f.write(f\"5. Dados limpos: {df_clean.shape[0]:,} registros, {df_clean.shape[1]} colunas\\\\n\")\n",
    "    f.write(f\"6. Engenharia de features: aplicado one-hot encoding\\\\n\")\n",
    "    f.write(f\"7. Dados processados: {df_encoded.shape[0]:,} registros, {df_encoded.shape[1]} colunas\\\\n\")\n",
    "    f.write(f\"8. Divis√£o treino/teste: {len(X_train):,}/{len(X_test):,} (80%/20%)\\\\n\\\\n\")\n",
    "    \n",
    "    f.write(\"DISTRIBUI√á√ÉO DA VARI√ÅVEL ALVO:\\\\n\")\n",
    "    f.write(f\"- Readmitidos em <30 dias: {y.sum():,} ({y.mean()*100:.1f}%)\\\\n\")\n",
    "    f.write(f\"- N√£o readmitidos em <30 dias: {len(y) - y.sum():,} ({(1-y.mean())*100:.1f}%)\\\\n\\\\n\")\n",
    "    \n",
    "    f.write(\"ARQUIVOS GERADOS:\\\\n\")\n",
    "    f.write(f\"- diabetic_data_clean.csv: Dados limpos intermedi√°rios\\\\n\")\n",
    "    f.write(f\"- diabetic_data_processed.csv: Dados completos processados\\\\n\")\n",
    "    f.write(f\"- X_train.csv, y_train.csv: Conjunto de treinamento\\\\n\")\n",
    "    f.write(f\"- X_test.csv, y_test.csv: Conjunto de teste\\\\n\")\n",
    "    f.write(f\"- processing_metadata.json: Metadados detalhados\\\\n\")\n",
    "\n",
    "print(f\"‚úÖ Resumo executivo salvo: {summary_path}\")\n",
    "\n",
    "# Visualiza√ß√£o final do pipeline\n",
    "print(f\"\\\\n\" + \"=\"*80)\n",
    "print(\"PIPELINE DE PROCESSAMENTO CONCLU√çDO\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "pipeline_summary = pd.DataFrame({\n",
    "    'Etapa': ['Dados Originais', 'Ap√≥s Limpeza', 'Ap√≥s Encoding', 'Treino', 'Teste'],\n",
    "    'Registros': [df_raw.shape[0], df_clean.shape[0], df_encoded.shape[0], X_train.shape[0], X_test.shape[0]],\n",
    "    'Colunas/Features': [df_raw.shape[1], df_clean.shape[1], df_encoded.shape[1], X_train.shape[1], X_test.shape[1]]\n",
    "})\n",
    "\n",
    "display(pipeline_summary)\n",
    "\n",
    "print(f\"\\\\nüéØ PR√ìXIMOS PASSOS:\")\n",
    "print(f\"1. ‚úÖ Dados preparados e prontos para Machine Learning\")\n",
    "print(f\"2. ü§ñ Implementar modelos de classifica√ß√£o (Random Forest, SVM, etc.)\")\n",
    "print(f\"3. üìä Avaliar performance dos modelos (accuracy, precision, recall, F1)\")\n",
    "print(f\"4. üîß Fazer tuning de hiperpar√¢metros\")\n",
    "print(f\"5. üìà Interpretar resultados e features mais importantes\")\n",
    "\n",
    "print(f\"\\\\n‚úÖ Todos os dados foram processados e salvos com sucesso!\")\n",
    "print(f\"üíæ Localiza√ß√£o dos arquivos: {os.path.abspath(output_dir)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c5b12b",
   "metadata": {},
   "source": [
    "## 5. An√°lise dos Mapeamentos de IDs\n",
    "\n",
    "Esta se√ß√£o analisa os dados enriquecidos com descri√ß√µes leg√≠veis dos c√≥digos IDs, melhorando a interpretabilidade dos dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a91ddbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar o utilit√°rio de mapeamentos\n",
    "from src.id_mapping_utils import IDMappingUtils\n",
    "\n",
    "# Carregar mapeamentos\n",
    "print(\"Carregando mapeamentos de IDs...\")\n",
    "id_mapper = IDMappingUtils()\n",
    "mappings = id_mapper.load_mappings()\n",
    "\n",
    "# Exibir resumo dos mapeamentos\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RESUMO DOS MAPEAMENTOS DISPON√çVEIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "summary = id_mapper.get_mapping_summary()\n",
    "for mapping_type, info in summary.items():\n",
    "    print(f\"\\n{mapping_type.upper()}:\")\n",
    "    print(f\"  N√∫mero de itens: {info['count']}\")\n",
    "    print(f\"  Range de IDs: {info['id_range']}\")\n",
    "    print(f\"  Exemplos: {info['sample_values']}\")\n",
    "\n",
    "# Validar mapeamentos com os dados atuais\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"VALIDA√á√ÉO DOS MAPEAMENTOS COM OS DADOS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "validation_report = id_mapper.validate_mappings(df_clean)\n",
    "for mapping_type, report in validation_report.items():\n",
    "    coverage = report['coverage_rate'] * 100\n",
    "    print(f\"\\n{mapping_type}:\")\n",
    "    print(f\"  IDs √∫nicos no dataset: {report['total_unique_ids']}\")\n",
    "    print(f\"  IDs com mapeamento: {report['mapped_ids']}\")\n",
    "    print(f\"  Taxa de cobertura: {coverage:.1f}%\")\n",
    "    if report['unmapped_ids']:\n",
    "        print(f\"  IDs n√£o mapeados: {report['unmapped_ids']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0799e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicar mapeamentos aos dados limpos\n",
    "print(\"Aplicando mapeamentos aos dados...\")\n",
    "df_enriched = id_mapper.apply_mappings_to_dataframe(df_clean)\n",
    "\n",
    "print(f\"\\nDimens√µes antes dos mapeamentos: {df_clean.shape}\")\n",
    "print(f\"Dimens√µes ap√≥s mapeamentos: {df_enriched.shape}\")\n",
    "\n",
    "# Verificar novas colunas criadas\n",
    "desc_columns = [col for col in df_enriched.columns if col.endswith('_desc')]\n",
    "print(f\"\\nNovas colunas descritivas criadas: {desc_columns}\")\n",
    "\n",
    "# Exibir amostra dos dados enriquecidos\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"AMOSTRA DOS DADOS ENRIQUECIDOS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Mostrar compara√ß√£o entre IDs e descri√ß√µes\n",
    "for col in desc_columns:\n",
    "    id_col = col.replace('_desc', '_id')\n",
    "    if id_col in df_enriched.columns:\n",
    "        print(f\"\\n{col.upper()}:\")\n",
    "        comparison = df_enriched[[id_col, col]].drop_duplicates().sort_values(id_col)\n",
    "        print(comparison.head(10))\n",
    "\n",
    "# An√°lise estat√≠stica das categorias com descri√ß√µes\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"AN√ÅLISE ESTAT√çSTICA COM DESCRI√á√ïES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for col in desc_columns:\n",
    "    print(f\"\\nDistribui√ß√£o de {col}:\")\n",
    "    distribution = df_enriched[col].value_counts()\n",
    "    print(distribution.head(10))\n",
    "    \n",
    "    # Taxa de readmiss√£o por categoria\n",
    "    if 'target' in df_enriched.columns:\n",
    "        print(f\"\\nTaxa de readmiss√£o por {col}:\")\n",
    "        readmission_rate = df_enriched.groupby(col)['target'].agg(['count', 'mean']).round(3)\n",
    "        readmission_rate.columns = ['Total_Casos', 'Taxa_Readmissao']\n",
    "        readmission_rate = readmission_rate.sort_values('Taxa_Readmissao', ascending=False)\n",
    "        print(readmission_rate.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23d5194",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar visualiza√ß√µes dos dados mapeados\n",
    "print(\"Criando visualiza√ß√µes dos dados enriquecidos...\")\n",
    "\n",
    "# Configurar o estilo dos gr√°ficos\n",
    "plt.style.use('seaborn-v0_8')\n",
    "fig = plt.figure(figsize=(20, 15))\n",
    "\n",
    "# 1. Distribui√ß√£o dos Tipos de Admiss√£o\n",
    "if 'admission_type_desc' in df_enriched.columns:\n",
    "    plt.subplot(3, 2, 1)\n",
    "    admission_counts = df_enriched['admission_type_desc'].value_counts()\n",
    "    plt.pie(admission_counts.values, labels=admission_counts.index, autopct='%1.1f%%')\n",
    "    plt.title('Distribui√ß√£o dos Tipos de Admiss√£o', fontsize=14, fontweight='bold')\n",
    "\n",
    "# 2. Taxa de Readmiss√£o por Tipo de Admiss√£o\n",
    "if 'admission_type_desc' in df_enriched.columns:\n",
    "    plt.subplot(3, 2, 2)\n",
    "    admission_readmission = df_enriched.groupby('admission_type_desc')['target'].mean().sort_values(ascending=False)\n",
    "    bars = plt.bar(range(len(admission_readmission)), admission_readmission.values)\n",
    "    plt.xticks(range(len(admission_readmission)), admission_readmission.index, rotation=45, ha='right')\n",
    "    plt.ylabel('Taxa de Readmiss√£o')\n",
    "    plt.title('Taxa de Readmiss√£o por Tipo de Admiss√£o', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # Adicionar valores nas barras\n",
    "    for i, v in enumerate(admission_readmission.values):\n",
    "        plt.text(i, v + 0.005, f'{v:.3f}', ha='center', va='bottom')\n",
    "\n",
    "# 3. Top 10 Disposi√ß√µes de Alta\n",
    "if 'discharge_disposition_desc' in df_enriched.columns:\n",
    "    plt.subplot(3, 2, 3)\n",
    "    top_discharge = df_enriched['discharge_disposition_desc'].value_counts().head(10)\n",
    "    plt.barh(range(len(top_discharge)), top_discharge.values)\n",
    "    plt.yticks(range(len(top_discharge)), [label[:30] + '...' if len(label) > 30 else label \n",
    "                                          for label in top_discharge.index])\n",
    "    plt.xlabel('N√∫mero de Casos')\n",
    "    plt.title('Top 10 Disposi√ß√µes de Alta', fontsize=14, fontweight='bold')\n",
    "\n",
    "# 4. Taxa de Readmiss√£o por Fonte de Admiss√£o\n",
    "if 'admission_source_desc' in df_enriched.columns:\n",
    "    plt.subplot(3, 2, 4)\n",
    "    source_readmission = df_enriched.groupby('admission_source_desc')['target'].agg(['count', 'mean'])\n",
    "    # Filtrar apenas fontes com pelo menos 100 casos\n",
    "    source_readmission = source_readmission[source_readmission['count'] >= 100]\n",
    "    source_readmission = source_readmission.sort_values('mean', ascending=False)\n",
    "    \n",
    "    bars = plt.bar(range(len(source_readmission)), source_readmission['mean'].values)\n",
    "    plt.xticks(range(len(source_readmission)), \n",
    "               [label[:15] + '...' if len(label) > 15 else label \n",
    "                for label in source_readmission.index], \n",
    "               rotation=45, ha='right')\n",
    "    plt.ylabel('Taxa de Readmiss√£o')\n",
    "    plt.title('Taxa de Readmiss√£o por Fonte de Admiss√£o (‚â•100 casos)', fontsize=14, fontweight='bold')\n",
    "\n",
    "# 5. Heatmap de correla√ß√£o entre vari√°veis categ√≥ricas mapeadas\n",
    "if len(desc_columns) >= 2:\n",
    "    plt.subplot(3, 2, 5)\n",
    "    # Criar matriz de correla√ß√£o para vari√°veis categ√≥ricas\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    \n",
    "    encoded_data = pd.DataFrame()\n",
    "    for col in desc_columns[:3]:  # Usar apenas as 3 primeiras para evitar sobrecarga\n",
    "        if col in df_enriched.columns:\n",
    "            le = LabelEncoder()\n",
    "            encoded_data[col] = le.fit_transform(df_enriched[col].fillna('Missing'))\n",
    "    \n",
    "    if not encoded_data.empty:\n",
    "        correlation_matrix = encoded_data.corr()\n",
    "        sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0)\n",
    "        plt.title('Correla√ß√£o entre Vari√°veis Categ√≥ricas Mapeadas', fontsize=14, fontweight='bold')\n",
    "\n",
    "# 6. Distribui√ß√£o de Readmiss√£o por combina√ß√£o de categorias\n",
    "if 'admission_type_desc' in df_enriched.columns and 'discharge_disposition_desc' in df_enriched.columns:\n",
    "    plt.subplot(3, 2, 6)\n",
    "    # Criar crosstab das duas principais categorias\n",
    "    cross_tab = pd.crosstab(df_enriched['admission_type_desc'], \n",
    "                           df_enriched['discharge_disposition_desc'], \n",
    "                           normalize='index')\n",
    "    # Mostrar apenas as 5 disposi√ß√µes mais comuns\n",
    "    top_dispositions = df_enriched['discharge_disposition_desc'].value_counts().head(5).index\n",
    "    cross_tab_filtered = cross_tab[top_dispositions]\n",
    "    \n",
    "    sns.heatmap(cross_tab_filtered, annot=True, fmt='.2f', cmap='Blues')\n",
    "    plt.title('Propor√ß√£o de Disposi√ß√µes de Alta por Tipo de Admiss√£o', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Disposi√ß√£o de Alta')\n",
    "    plt.ylabel('Tipo de Admiss√£o')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Estat√≠sticas resumidas dos mapeamentos\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ESTAT√çSTICAS RESUMIDAS DOS MAPEAMENTOS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for col in desc_columns:\n",
    "    print(f\"\\n{col.upper()}:\")\n",
    "    print(f\"  Categorias √∫nicas: {df_enriched[col].nunique()}\")\n",
    "    print(f\"  Categoria mais comum: {df_enriched[col].mode().iloc[0]}\")\n",
    "    print(f\"  Valores nulos: {df_enriched[col].isnull().sum()}\")\n",
    "    \n",
    "    if 'target' in df_enriched.columns:\n",
    "        most_risky = df_enriched.groupby(col)['target'].mean().sort_values(ascending=False).iloc[0]\n",
    "        most_risky_category = df_enriched.groupby(col)['target'].mean().sort_values(ascending=False).index[0]\n",
    "        print(f\"  Categoria com maior risco: {most_risky_category} ({most_risky:.3f})\")\n",
    "\n",
    "print(f\"\\nDados enriquecidos salvos em mem√≥ria com {df_enriched.shape[0]} registros e {df_enriched.shape[1]} colunas.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
