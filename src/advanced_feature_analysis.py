"""
An√°lise Avan√ßada de Features - Sistema Completo de Feature Engineering

Este m√≥dulo implementa t√©cnicas avan√ßadas para an√°lise, sele√ß√£o e interpreta√ß√£o
de features em modelos de Machine Learning para predi√ß√£o m√©dica.

Funcionalidades:
- An√°lise SHAP para interpretabilidade expl√≠cita
- Sele√ß√£o recursiva de features com valida√ß√£o cruzada
- An√°lise de import√¢ncia por permuta√ß√£o
- Detec√ß√£o de correla√ß√µes e multicolinearidade
- An√°lise de intera√ß√µes entre features
- Feature importance stability analysis
- An√°lise de contribui√ß√£o individual por paciente

Autor: Thalles Felipe Rodrigues de Almeida Santos
Projeto: Predi√ß√£o de Readmiss√£o Hospitalar em Pacientes com Diabetes
Data: Agosto 2025
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
from datetime import datetime
from typing import Dict, List, Tuple, Any, Optional
import joblib
import json

warnings.filterwarnings('ignore')

# Configurar matplotlib
import matplotlib
matplotlib.use('Agg')
plt.ioff()

# Scikit-learn imports
from sklearn.feature_selection import (
    RFECV, SelectKBest, f_classif, chi2, mutual_info_classif,
    VarianceThreshold, SelectFromModel
)
from sklearn.inspection import permutation_importance
from sklearn.model_selection import StratifiedKFold, cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import roc_auc_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.base import clone

# SHAP para interpretabilidade
try:
    import shap
    SHAP_AVAILABLE = True
except ImportError:
    SHAP_AVAILABLE = False
    print("‚ö†Ô∏è  SHAP n√£o dispon√≠vel. Instale com: pip install shap")

# Estat√≠sticas avan√ßadas
from scipy import stats
from scipy.stats import chi2_contingency, spearmanr, pearsonr
from statsmodels.stats.outliers_influence import variance_inflation_factor


class AdvancedFeatureAnalyzer:
    """
    Classe para an√°lise avan√ßada e completa de features
    """
    
    def __init__(self, random_state: int = 42):
        """
        Inicializa o analisador de features
        
        Args:
            random_state: Seed para reprodutibilidade
        """
        self.random_state = random_state
        self.feature_rankings = {}
        self.importance_results = {}
        self.shap_values = None
        self.selected_features = None
        
    def comprehensive_feature_importance(self, models_dict, X_train, y_train, X_test, y_test):
        """
        An√°lise abrangente de import√¢ncia usando m√∫ltiplos m√©todos
        
        Args:
            models_dict: Dicion√°rio com modelos treinados {'nome': modelo}
            X_train: Features de treino
            y_train: Target de treino
            X_test: Features de teste
            y_test: Target de teste
            
        Returns:
            dict: Resultados completos de import√¢ncia
        """
        print("üîç Executando An√°lise Abrangente de Import√¢ncia de Features...")
        
        importance_results = {}
        feature_names = X_train.columns.tolist()
        
        for model_name, model in models_dict.items():
            print(f"   Analisando modelo: {model_name}")
            
            model_results = {
                'feature_names': feature_names,
                'methods': {}
            }
            
            # 1. Import√¢ncia nativa do modelo (se dispon√≠vel)
            if hasattr(model, 'feature_importances_'):
                model_results['methods']['native_importance'] = {
                    'values': model.feature_importances_.tolist(),
                    'ranking': np.argsort(model.feature_importances_)[::-1].tolist()
                }
            
            # 2. Import√¢ncia por coeficientes (para modelos lineares)
            if hasattr(model, 'coef_'):
                coef_abs = np.abs(model.coef_[0])
                model_results['methods']['coefficient_importance'] = {
                    'values': coef_abs.tolist(),
                    'ranking': np.argsort(coef_abs)[::-1].tolist()
                }
            
            # 3. Import√¢ncia por permuta√ß√£o
            perm_importance = permutation_importance(
                model, X_test, y_test,
                n_repeats=10,
                random_state=self.random_state,
                scoring='roc_auc'
            )
            
            model_results['methods']['permutation_importance'] = {
                'values': perm_importance.importances_mean.tolist(),
                'std': perm_importance.importances_std.tolist(),
                'ranking': np.argsort(perm_importance.importances_mean)[::-1].tolist()
            }
            
            # 4. An√°lise SHAP (se dispon√≠vel)
            if SHAP_AVAILABLE:
                try:
                    shap_results = self._calculate_shap_importance(model, X_test, model_name)
                    if shap_results:
                        model_results['methods']['shap_importance'] = shap_results
                except Exception as e:
                    print(f"   ‚ö†Ô∏è  Erro no SHAP para {model_name}: {e}")
            
            importance_results[model_name] = model_results
        
        # 5. M√©todos agn√≥sticos de modelo
        print("   Executando m√©todos agn√≥sticos...")
        
        # Correla√ß√£o com target
        correlations = []
        for col in feature_names:
            if X_train[col].dtype in ['int64', 'float64']:
                corr, _ = pearsonr(X_train[col], y_train)
                correlations.append(abs(corr))
            else:
                correlations.append(0.0)
        
        importance_results['model_agnostic'] = {
            'feature_names': feature_names,
            'methods': {
                'correlation_with_target': {
                    'values': correlations,
                    'ranking': np.argsort(correlations)[::-1].tolist()
                }
            }
        }
        
        # Mutual Information
        mi_scores = mutual_info_classif(X_train, y_train, random_state=self.random_state)
        importance_results['model_agnostic']['methods']['mutual_information'] = {
            'values': mi_scores.tolist(),
            'ranking': np.argsort(mi_scores)[::-1].tolist()
        }
        
        # Univariate Statistical Tests
        f_scores, _ = f_classif(X_train, y_train)
        importance_results['model_agnostic']['methods']['f_statistic'] = {
            'values': f_scores.tolist(),
            'ranking': np.argsort(f_scores)[::-1].tolist()
        }
        
        self.importance_results = importance_results
        return importance_results
    
    def recursive_feature_elimination_cv(self, model, X_train, y_train, 
                                       min_features=10, step=1, cv_folds=5):
        """
        Sele√ß√£o recursiva de features com valida√ß√£o cruzada
        
        Args:
            model: Modelo base para sele√ß√£o
            X_train: Features de treino
            y_train: Target de treino
            min_features: N√∫mero m√≠nimo de features
            step: Passo de elimina√ß√£o
            cv_folds: Folds para valida√ß√£o cruzada
            
        Returns:
            dict: Resultados da sele√ß√£o recursiva
        """
        print("üîÑ Executando Sele√ß√£o Recursiva de Features com CV...")
        
        # RFECV
        cv_strategy = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=self.random_state)
        
        rfecv = RFECV(
            estimator=clone(model),
            step=step,
            cv=cv_strategy,
            scoring='roc_auc',
            min_features_to_select=min_features,
            n_jobs=-1
        )
        
        rfecv.fit(X_train, y_train)
        
        # An√°lise detalhada dos resultados
        rfe_results = {
            'optimal_features': rfecv.n_features_,
            'feature_ranking': rfecv.ranking_.tolist(),
            'cv_scores': rfecv.cv_results_['mean_test_score'].tolist(),
            'cv_scores_std': rfecv.cv_results_['std_test_score'].tolist(),
            'selected_features': X_train.columns[rfecv.support_].tolist(),
            'eliminated_features': X_train.columns[~rfecv.support_].tolist(),
            'feature_elimination_order': [],
            'score_improvement': []
        }
        
        # Calcular ordem de elimina√ß√£o e melhoria de score
        for n_features in range(len(X_train.columns), min_features - 1, -step):
            idx = len(X_train.columns) - n_features
            if idx < len(rfecv.cv_results_['mean_test_score']):
                score = rfecv.cv_results_['mean_test_score'][idx]
                rfe_results['score_improvement'].append(score)
        
        print(f"   ‚úÖ Features selecionadas: {rfecv.n_features_}/{len(X_train.columns)}")
        print(f"   üìà Score CV: {rfecv.cv_results_['mean_test_score'][rfecv.n_features_ - min_features]:.4f}")
        
        self.selected_features = rfecv.support_
        return rfe_results
    
    def multicollinearity_analysis(self, X_train):
        """
        An√°lise detalhada de multicolinearidade
        
        Args:
            X_train: Features de treino
            
        Returns:
            dict: An√°lise de multicolinearidade
        """
        print("üìä Executando An√°lise de Multicolinearidade...")
        
        # Selecionar apenas features num√©ricas
        numeric_features = X_train.select_dtypes(include=[np.number])
        
        if len(numeric_features.columns) == 0:
            return {'error': 'Nenhuma feature num√©rica encontrada'}
        
        # Matriz de correla√ß√£o
        correlation_matrix = numeric_features.corr()
        
        # Encontrar pares altamente correlacionados
        high_corr_pairs = []
        correlation_threshold = 0.8
        
        for i in range(len(correlation_matrix.columns)):
            for j in range(i + 1, len(correlation_matrix.columns)):
                corr_value = abs(correlation_matrix.iloc[i, j])
                if corr_value > correlation_threshold:
                    high_corr_pairs.append({
                        'feature_1': correlation_matrix.columns[i],
                        'feature_2': correlation_matrix.columns[j],
                        'correlation': corr_value
                    })
        
        # Variance Inflation Factor (VIF)
        vif_data = []
        try:
            for i, feature in enumerate(numeric_features.columns):
                vif_value = variance_inflation_factor(numeric_features.values, i)
                vif_data.append({
                    'feature': feature,
                    'vif': vif_value
                })
        except Exception as e:
            print(f"   ‚ö†Ô∏è  Erro no c√°lculo VIF: {e}")
            vif_data = []
        
        multicollinearity_results = {
            'correlation_matrix': correlation_matrix.to_dict(),
            'high_correlation_pairs': high_corr_pairs,
            'vif_analysis': vif_data,
            'recommendations': []
        }
        
        # Gerar recomenda√ß√µes
        if high_corr_pairs:
            multicollinearity_results['recommendations'].append(
                f"Encontrados {len(high_corr_pairs)} pares com correla√ß√£o > {correlation_threshold}"
            )
        
        high_vif_features = [item for item in vif_data if item['vif'] > 10]
        if high_vif_features:
            multicollinearity_results['recommendations'].append(
                f"{len(high_vif_features)} features com VIF > 10 (poss√≠vel multicolinearidade)"
            )
        
        return multicollinearity_results
    
    def feature_interaction_analysis(self, model, X_train, y_train, top_features=10):
        """
        An√°lise de intera√ß√µes entre features
        
        Args:
            model: Modelo treinado
            X_train: Features de treino
            y_train: Target de treino
            top_features: N√∫mero de top features para analisar intera√ß√µes
            
        Returns:
            dict: An√°lise de intera√ß√µes
        """
        print("üîó Executando An√°lise de Intera√ß√µes entre Features...")
        
        # Selecionar top features baseado na import√¢ncia
        if hasattr(model, 'feature_importances_'):
            top_indices = np.argsort(model.feature_importances_)[-top_features:]
            top_feature_names = X_train.columns[top_indices].tolist()
        else:
            # Use correla√ß√£o como fallback
            correlations = []
            for col in X_train.columns:
                if X_train[col].dtype in ['int64', 'float64']:
                    corr, _ = pearsonr(X_train[col], y_train)
                    correlations.append(abs(corr))
                else:
                    correlations.append(0.0)
            
            top_indices = np.argsort(correlations)[-top_features:]
            top_feature_names = X_train.columns[top_indices].tolist()
        
        # An√°lise de intera√ß√µes par a par
        interaction_results = {
            'analyzed_features': top_feature_names,
            'pairwise_interactions': [],
            'interaction_importance': {}
        }
        
        base_score = cross_val_score(model, X_train[top_feature_names], y_train, 
                                   cv=3, scoring='roc_auc').mean()
        
        for i in range(len(top_feature_names)):
            for j in range(i + 1, len(top_feature_names)):
                feature_1 = top_feature_names[i]
                feature_2 = top_feature_names[j]
                
                # Criar feature de intera√ß√£o
                X_interaction = X_train[top_feature_names].copy()
                
                # Multiplica√ß√£o para features num√©ricas
                if (X_train[feature_1].dtype in ['int64', 'float64'] and 
                    X_train[feature_2].dtype in ['int64', 'float64']):
                    
                    interaction_name = f"{feature_1}_x_{feature_2}"
                    X_interaction[interaction_name] = X_train[feature_1] * X_train[feature_2]
                    
                    # Avaliar melhoria com intera√ß√£o
                    interaction_score = cross_val_score(
                        model, X_interaction, y_train, cv=3, scoring='roc_auc'
                    ).mean()
                    
                    improvement = interaction_score - base_score
                    
                    interaction_results['pairwise_interactions'].append({
                        'feature_1': feature_1,
                        'feature_2': feature_2,
                        'interaction_name': interaction_name,
                        'base_score': base_score,
                        'interaction_score': interaction_score,
                        'improvement': improvement
                    })
        
        # Ordenar por melhoria
        interaction_results['pairwise_interactions'].sort(
            key=lambda x: x['improvement'], reverse=True
        )
        
        return interaction_results
    
    def feature_stability_analysis(self, model, X_train, y_train, n_iterations=20):
        """
        An√°lise de estabilidade da import√¢ncia das features
        
        Args:
            model: Modelo base
            X_train: Features de treino
            y_train: Target de treino
            n_iterations: N√∫mero de itera√ß√µes para an√°lise
            
        Returns:
            dict: An√°lise de estabilidade
        """
        print(f"üìà Executando An√°lise de Estabilidade das Features ({n_iterations} itera√ß√µes)...")
        
        feature_names = X_train.columns.tolist()
        importance_matrix = []
        
        for i in range(n_iterations):
            # Bootstrap sampling
            n_samples = len(X_train)
            bootstrap_indices = np.random.choice(n_samples, size=n_samples, replace=True)
            X_bootstrap = X_train.iloc[bootstrap_indices]
            y_bootstrap = y_train.iloc[bootstrap_indices]
            
            # Treinar modelo
            model_temp = clone(model)
            model_temp.fit(X_bootstrap, y_bootstrap)
            
            # Extrair import√¢ncia
            if hasattr(model_temp, 'feature_importances_'):
                importance_matrix.append(model_temp.feature_importances_)
            else:
                # Usar import√¢ncia por permuta√ß√£o como fallback
                perm_imp = permutation_importance(
                    model_temp, X_bootstrap, y_bootstrap,
                    n_repeats=3, random_state=i
                )
                importance_matrix.append(perm_imp.importances_mean)
        
        importance_matrix = np.array(importance_matrix)
        
        # Calcular estat√≠sticas de estabilidade
        stability_results = {
            'feature_names': feature_names,
            'importance_stats': {
                'mean': importance_matrix.mean(axis=0).tolist(),
                'std': importance_matrix.std(axis=0).tolist(),
                'min': importance_matrix.min(axis=0).tolist(),
                'max': importance_matrix.max(axis=0).tolist(),
                'coefficient_variation': (importance_matrix.std(axis=0) / 
                                        (importance_matrix.mean(axis=0) + 1e-8)).tolist()
            },
            'ranking_stability': {},
            'top_stable_features': [],
            'unstable_features': []
        }
        
        # An√°lise de estabilidade do ranking
        rankings = []
        for iteration_importance in importance_matrix:
            ranking = np.argsort(iteration_importance)[::-1]
            rankings.append(ranking)
        
        rankings = np.array(rankings)
        
        # Calcular estabilidade do ranking para cada feature
        for i, feature in enumerate(feature_names):
            positions = []
            for ranking in rankings:
                position = np.where(ranking == i)[0][0]
                positions.append(position)
            
            stability_results['ranking_stability'][feature] = {
                'mean_position': np.mean(positions),
                'std_position': np.std(positions),
                'position_range': (int(np.min(positions)), int(np.max(positions)))
            }
        
        # Identificar features est√°veis e inst√°veis
        cv_threshold = 0.5  # Coeficiente de varia√ß√£o
        for i, feature in enumerate(feature_names):
            cv = stability_results['importance_stats']['coefficient_variation'][i]
            std_pos = stability_results['ranking_stability'][feature]['std_position']
            
            if cv < cv_threshold and std_pos < 5:
                stability_results['top_stable_features'].append(feature)
            elif cv > 1.0 or std_pos > 10:
                stability_results['unstable_features'].append(feature)
        
        return stability_results
    
    def _calculate_shap_importance(self, model, X_test, model_name):
        """
        Calcula import√¢ncia SHAP para interpretabilidade
        
        Args:
            model: Modelo treinado
            X_test: Features de teste
            model_name: Nome do modelo
            
        Returns:
            dict: Resultados SHAP
        """
        try:
            # Selecionar explainer apropriado
            if hasattr(model, 'predict_proba'):
                if 'forest' in model_name.lower() or 'tree' in model_name.lower():
                    explainer = shap.TreeExplainer(model)
                else:
                    explainer = shap.KernelExplainer(model.predict_proba, X_test.sample(100))
            else:
                explainer = shap.KernelExplainer(model.predict, X_test.sample(100))
            
            # Calcular SHAP values (limitando a 500 amostras para performance)
            sample_size = min(500, len(X_test))
            X_sample = X_test.sample(n=sample_size, random_state=self.random_state)
            
            shap_values = explainer.shap_values(X_sample)
            
            # Para classifica√ß√£o bin√°ria, pegar valores da classe positiva
            if isinstance(shap_values, list) and len(shap_values) == 2:
                shap_values = shap_values[1]
            
            # Calcular import√¢ncia m√©dia absoluta
            mean_abs_shap = np.mean(np.abs(shap_values), axis=0)
            
            shap_results = {
                'values': mean_abs_shap.tolist(),
                'ranking': np.argsort(mean_abs_shap)[::-1].tolist(),
                'sample_size': sample_size
            }
            
            # Salvar gr√°ficos SHAP
            self._save_shap_plots(shap_values, X_sample, model_name)
            
            return shap_results
            
        except Exception as e:
            print(f"   ‚ö†Ô∏è  Erro no c√°lculo SHAP: {e}")
            return None
    
    def _save_shap_plots(self, shap_values, X_sample, model_name):
        """
        Salva gr√°ficos SHAP
        """
        try:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            
            # Summary plot
            plt.figure(figsize=(10, 8))
            shap.summary_plot(shap_values, X_sample, show=False, max_display=20)
            plt.tight_layout()
            plt.savefig(f'results/shap_summary_{model_name}_{timestamp}.png', 
                       dpi=300, bbox_inches='tight')
            plt.close()
            
            # Bar plot
            plt.figure(figsize=(10, 6))
            shap.summary_plot(shap_values, X_sample, plot_type="bar", show=False, max_display=15)
            plt.tight_layout()
            plt.savefig(f'results/shap_bar_{model_name}_{timestamp}.png', 
                       dpi=300, bbox_inches='tight')
            plt.close()
            
        except Exception as e:
            print(f"   ‚ö†Ô∏è  Erro ao salvar gr√°ficos SHAP: {e}")
    
    def generate_feature_report(self, importance_results, rfe_results, 
                              multicollinearity_results, interaction_results, 
                              stability_results, model_name):
        """
        Gera relat√≥rio completo de an√°lise de features
        
        Args:
            importance_results: Resultados de import√¢ncia
            rfe_results: Resultados RFE
            multicollinearity_results: An√°lise multicolinearidade
            interaction_results: An√°lise de intera√ß√µes
            stability_results: An√°lise de estabilidade
            model_name: Nome do modelo
            
        Returns:
            str: Relat√≥rio formatado
        """
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        
        report = f"""
üî¨ RELAT√ìRIO AVAN√áADO DE AN√ÅLISE DE FEATURES - {model_name.upper()}
============================================================

üìÖ Data/Hora: {timestamp}
üîç An√°lise: Import√¢ncia, Sele√ß√£o, Intera√ß√µes e Estabilidade

üìä SELE√á√ÉO RECURSIVA DE FEATURES (RFECV):
------------------------------
Features Originais: {len(rfe_results['feature_ranking'])}
Features Selecionadas: {rfe_results['optimal_features']}
Score CV √ìtimo: {max(rfe_results['cv_scores']):.4f}

Top 10 Features Selecionadas:
"""
        
        for i, feature in enumerate(rfe_results['selected_features'][:10]):
            report += f"  {i+1:2d}. {feature}\n"
        
        report += f"""
üîó AN√ÅLISE DE MULTICOLINEARIDADE:
------------------------------
Pares Altamente Correlacionados: {len(multicollinearity_results.get('high_correlation_pairs', []))}"""
        
        if 'vif_analysis' in multicollinearity_results:
            high_vif = [item for item in multicollinearity_results['vif_analysis'] if item['vif'] > 10]
            report += f"\nFeatures com VIF > 10: {len(high_vif)}"
        
        report += f"""

üîó AN√ÅLISE DE INTERA√á√ïES:
------------------------------
Features Analisadas: {len(interaction_results['analyzed_features'])}
Intera√ß√µes Testadas: {len(interaction_results['pairwise_interactions'])}

Top 5 Intera√ß√µes Mais Importantes:"""
        
        for i, interaction in enumerate(interaction_results['pairwise_interactions'][:5]):
            report += f"""
  {i+1}. {interaction['feature_1']} √ó {interaction['feature_2']}
     Melhoria: {interaction['improvement']:+.4f}"""
        
        report += f"""

üìà AN√ÅLISE DE ESTABILIDADE:
------------------------------
Features Est√°veis: {len(stability_results['top_stable_features'])}
Features Inst√°veis: {len(stability_results['unstable_features'])}

Top 10 Features Mais Est√°veis:"""
        
        stable_features = stability_results['top_stable_features'][:10]
        for i, feature in enumerate(stable_features):
            cv = stability_results['importance_stats']['coefficient_variation'][
                stability_results['feature_names'].index(feature)
            ]
            report += f"\n  {i+1:2d}. {feature} (CV: {cv:.3f})"
        
        if stability_results['unstable_features']:
            report += f"""

‚ö†Ô∏è  Features Inst√°veis (requem aten√ß√£o):"""
            for feature in stability_results['unstable_features'][:5]:
                cv = stability_results['importance_stats']['coefficient_variation'][
                    stability_results['feature_names'].index(feature)
                ]
                report += f"\n  ‚Ä¢ {feature} (CV: {cv:.3f})"
        
        report += f"""

‚úÖ RECOMENDA√á√ïES:
------------------------------
‚Ä¢ Usar as {rfe_results['optimal_features']} features selecionadas pelo RFECV
‚Ä¢ Considerar remo√ß√£o de features com alta multicolinearidade
‚Ä¢ Avaliar inclus√£o das top 3 intera√ß√µes de features
‚Ä¢ Monitorar estabilidade das features inst√°veis em produ√ß√£o
‚Ä¢ Aplicar regulariza√ß√£o para features com alta variabilidade

"""
        return report
    
    def save_analysis_results(self, all_results, model_name, timestamp=None):
        """
        Salva todos os resultados da an√°lise
        
        Args:
            all_results: Dicion√°rio com todos os resultados
            model_name: Nome do modelo
            timestamp: Timestamp opcional
        """
        if timestamp is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        filename = f"results/feature_analysis_{model_name}_{timestamp}.json"
        
        # Converter numpy arrays para listas
        def convert_numpy(obj):
            if isinstance(obj, np.ndarray):
                return obj.tolist()
            elif isinstance(obj, np.integer):
                return int(obj)
            elif isinstance(obj, np.floating):
                return float(obj)
            elif isinstance(obj, dict):
                return {key: convert_numpy(value) for key, value in obj.items()}
            elif isinstance(obj, list):
                return [convert_numpy(item) for item in obj]
            return obj
        
        results_serializable = convert_numpy(all_results)
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(results_serializable, f, indent=2, ensure_ascii=False)
        
        print(f"üíæ An√°lise salva em: {filename}")
